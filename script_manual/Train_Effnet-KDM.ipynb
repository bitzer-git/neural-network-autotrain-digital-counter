{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruduction\n",
    "\n",
    "The Notebook tries to improve the models of https://github.com/jomjol/neural-network-autotrain-digital-counter\n",
    "\n",
    "#### Review of the current models\n",
    "\n",
    "The models are small CNN models to run on edge-AI device ESP32-cam. The dataset is very small with round about 1000 images.\n",
    "\n",
    "In the autotrain script the models will be trained multiple times with the same dataset. Because of the shuffling the complete dataset will be used as train data. With only one run the training achieves not the 99% accuracy.\n",
    "\n",
    "Thats why the notebooks tries to creates a small, but better model.\n",
    "\n",
    "### Mixed datasets\n",
    "\n",
    "Because zifferdata is a very small dataset, char74k will be used to raise up the count of input images.\n",
    "\n",
    "http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/\n",
    "\n",
    "chars74 has a small set of numbers (and letters) obtained from natural images. It has also a larger dataset of computer fonts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Transfer learning not used (why)\n",
    "\n",
    "Because of the NaN-category i tried to train first the numbers 0-9 and using transfer learning with fine tune, described here https://keras.io/guides/transfer_learning/. This i tried at first with no success. \n",
    "\n",
    "### Knowledge Distillation\n",
    "\n",
    "To reach >99% on a smal model i tried here knowledge distillation (https://keras.io/examples/vision/knowledge_distillation/ ). It uses a bigger teachner model to train a smaller (edgeAI) student model.\n",
    "\n",
    "### Training steps\n",
    "\n",
    "\n",
    "Now the data of ziffer_raw and the two chars74k datasets will be mixed for the first training.\n",
    "\n",
    "In a second step only the ziffer_raw data will be uses (the same validation-split is here needed to ensure that train and validation every time the same).\n",
    "\n",
    "First we learn with a dataset like described above. If the learning reaches >99% in a second step the smaller ziffer dataset will be trained again. Here only the last (two) layer will be trained with a lower learning-rate to avoid overfitting.\n",
    "\n",
    "\n",
    "A third step is a fine tune step with much less learning-rate but all layers will be trained.\n",
    "\n",
    "The training of every step will be stopped before the model is overfitted. (Validation-Loss is much higher than training loss).\n",
    "\n",
    "\n",
    "### Preparing the training\n",
    "* First all libraries are loaded\n",
    "    * It is assumed, that they are installed during the Python setup\n",
    "* matplotlib is set to print the output inline in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########### Basic Parameters for Running: ################################\n",
    "    \n",
    "TFliteNamingAndVersion = \"knd\"      # Used for tflite Filename\n",
    "Training_Percentage = 0.2           \n",
    "Epoch_Anz = 80\n",
    "nb_classes = 11                     # move to 1. step\n",
    "input_shape = (32, 20,3)\n",
    "\n",
    "##########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##VD## pip3 install tensorflow_datasets \n",
    "##VD## pip3 install opencv-python\n",
    "\n",
    "\n",
    "\n",
    "# imports\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Conv2D, MaxPool2D, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import History \n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data methods\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from pathlib import Path\n",
    "import tensorflow_datasets as tfds\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "def ziffer_data_files():\n",
    "    #dataset_ziffer_url = \"https://github.com/jomjol/neural-network-autotrain-digital-counter/archive/refs/heads/main.zip\"\n",
    "    \n",
    "    #ziffer_dir = get_file(origin=dataset_ziffer_url,\n",
    "    #                        fname='neural-network-autotrain-digital-counter-main.zip',\n",
    "    #                        archive_format='zip',\n",
    "    #                        extract=True)\n",
    "    #remove .zip                        \n",
    "    #ziffer_dir = ziffer_dir[:-4]\n",
    "    #return glob.glob(ziffer_dir + '/ziffer_raw/*.jpg')\n",
    "    Input_dir='../ziffer_raw'\n",
    "    return  glob.glob(Input_dir + '/*.jpg')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# load all data\n",
    "def ziffer_data(x_data, y_data, nb_classes):\n",
    "\n",
    "    files = ziffer_data_files()\n",
    "\n",
    "    for aktfile in files:\n",
    "        base = os.path.basename(aktfile)\n",
    "        target = base[0:1]\n",
    "        if target == \"N\":\n",
    "            category = 10                # NaN does not work --> convert to 10\n",
    "\n",
    "        else:\n",
    "            category = int(target)\n",
    "        test_image = Image.open(aktfile).resize((20, 32))\n",
    "        test_image = np.array(test_image, dtype=\"float32\")\n",
    "\n",
    "        # if only 10 classes, ignore the category 10\n",
    "        if (nb_classes>10 or category<10):\n",
    "            x_data.append(test_image)\n",
    "            y_data.append(np.array([category]))\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "###### read english number images (numbers in pictures)\n",
    "### see http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/\n",
    "def eng_char74k_numbers(x_data, y_data, nb_classes=None):\n",
    "    dataset_eng_url = \"http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/EnglishImg.tgz\"\n",
    "    data_eng_dir = get_file(origin=dataset_eng_url,\n",
    "                            fname='English',\n",
    "                            untar=True)\n",
    "\n",
    "    # Sample001-Sample011 are numbers\n",
    "    for i in range(1, 11):\n",
    "        files = glob.glob(data_eng_dir + '/Img/GoodImg/Bmp/Sample'+str(i).zfill(3)+ '/*.png')\n",
    "        for aktfile in files:\n",
    "            base = os.path.basename(aktfile)\n",
    "            target = base[4:6]\n",
    "            category = int(target)-1\n",
    "            if (category>10):\n",
    "                category=10\n",
    "            test_image = Image.open(aktfile).resize((20, 32)).convert(\"RGB\")\n",
    "            test_image = np.array(test_image, dtype=\"float32\")\n",
    "            x_data.append(test_image)\n",
    "            y_data.append(np.array([category]))\n",
    "    return x_data, y_data\n",
    "\n",
    "###### add font images (numbers in pictures)\n",
    "### see http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/\n",
    "def font_char74k_numbers(x_data, y_data, nb_classes=None):\n",
    "    dataset_fnt_url = \"http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/EnglishFnt.tgz\"\n",
    "    data_fnt_dir = get_file(origin=dataset_fnt_url,\n",
    "                                    fname='English/Fnt',\n",
    "                                    untar=True)\n",
    "    # Sample001-Sample011 are numbers\n",
    "    for i in range(1, 11):\n",
    "        files = glob.glob(data_fnt_dir + '/Sample'+str(i).zfill(3)+ '/*.png')\n",
    "        for aktfile in files:\n",
    "            base = os.path.basename(aktfile)\n",
    "            target = base[4:6]\n",
    "            category = int(target)-1\n",
    "            if (category>10):\n",
    "                category=10\n",
    "            test_image = Image.open(aktfile).resize((20, 32)).convert('RGB')\n",
    "            test_image = np.array(test_image, dtype=\"float32\")\n",
    "            x_data.append(test_image)\n",
    "            y_data.append(np.array([category]))\n",
    "    return x_data, y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dataset(images, labels, columns=12, rows=5):\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    columns = 12\n",
    "    rows = 5\n",
    "\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(labels[i])  # set title\n",
    "        plt.imshow((images[i].astype(np.uint8)))\n",
    "    plt.show()\n",
    "\n",
    "def plot_dataset_it(data_iter, columns=12, rows=5):\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    columns = 12\n",
    "    rows = 5\n",
    "\n",
    "    for i in range(1, columns*rows +1):\n",
    "        img, label = data_iter.next()\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(label[0])  # set title\n",
    "        plt.imshow((img[0].astype(np.uint8)))\n",
    "    plt.show()\n",
    "\n",
    "def plot_acc_loss(history, modelname=\"modelname\"):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(modelname)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    if \"loss\" in history.history:\n",
    "        ax1.plot(history.history['loss'])\n",
    "    if \"accuracy\" in history.history:\n",
    "        ax2.plot(history.history['accuracy'])\n",
    "    if \"val_loss\" in history.history:\n",
    "        ax1.plot(history.history['val_loss'])\n",
    "    if \"val_accuracy\" in history.history:\n",
    "        ax2.plot(history.history['val_accuracy'])\n",
    "    if \"student_loss\" in history.history:\n",
    "        ax1.plot(history.history['student_loss'])\n",
    "    if \"sparse_categorical_accuracy\" in history.history:\n",
    "        ax2.plot(history.history['sparse_categorical_accuracy'])\n",
    "    if \"val_sparse_categorical_accuracy\" in history.history:\n",
    "        ax2.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "    if \"student_accuracy\" in history.history:\n",
    "        ax2.plot(history.history['student_accuracy'])\n",
    "    if \"val_student_accuracy\" in history.history:\n",
    "        ax2.plot(history.history['val_student_accuracy'])\n",
    "    if \"distillation_loss\" in history.history:\n",
    "        ax1.plot(history.history['distillation_loss'])\n",
    "\n",
    "    ax1.set_title('model loss')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax1.legend(['train','eval'], loc='upper left')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.92,1])\n",
    "    plt.show()\n",
    "\n",
    "def plot_dist_acc_loss(history, modelname=\"modelname\"):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(modelname)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    ax1.plot(history.history['student_loss'])\n",
    "    ax2.plot(history.history['sparse_categorical_accuracy'])\n",
    "    ax1.plot(history.history['distillation_loss'])\n",
    "    \n",
    "    ax1.set_title('model loss')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    \n",
    "    ax1.legend(['student','distillation'], loc='upper left')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.92,1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_val_acc(models, history):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    styles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']\n",
    "    names = []\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for i, model in enumerate(models):\n",
    "        names.append(model._name)\n",
    "        if \"accuracy\" in history[i].history:\n",
    "             plt.plot(history[i].history['accuracy'],linestyle=styles[i])\n",
    "             plt.ylabel('accuracy')\n",
    "        if \"val_accuracy\" in history[i].history:\n",
    "             plt.plot(history[i].history['val_accuracy'],linestyle=styles[i])\n",
    "             plt.ylabel('val. accuracy')\n",
    "        if \"val_sparse_categorical_accuracy\" in history[i].history:\n",
    "             plt.plot(history[i].history['val_sparse_categorical_accuracy'],linestyle=styles[i])\n",
    "             plt.ylabel('val. sparse accuracy')\n",
    "        if \"val_studend_accuracy\" in history[i].history:\n",
    "             plt.plot(history[i].history['val_studend_accuracy'],linestyle=styles[i])\n",
    "             plt.ylabel('val. studend accuracy')\n",
    "\n",
    "             \n",
    "    plt.title('model validation accuracy')\n",
    "     \n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper left')\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.98,1])\n",
    "    plt.show()\n",
    "\n",
    "def printMaxHistory(history, modelname):\n",
    "    if \"accuracy\" in history.history:\n",
    "        acc = max(history.history['accuracy'])\n",
    "    if \"val_accuracy\" in history.history:\n",
    "         val_acc = max(history.history['val_accuracy']) \n",
    "    if \"sparse_categorical_accuracy\" in history.history:\n",
    "        acc = max(history.history['sparse_categorical_accuracy']) \n",
    "    if \"val_sparse_categorical_accuracy\" in history.history:\n",
    "        val_acc = max(history.history['val_sparse_categorical_accuracy']) \n",
    "    if \"studend_accuracy\" in history.history:\n",
    "        acc = max(history[i].history['studend_accuracy']) \n",
    "    if \"val_studend_accuracy\" in history.history:\n",
    "        val_acc = max(history.history['val_studend_accuracy']) \n",
    "            \n",
    "    \n",
    "    print(\"Model {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "        modelname,\n",
    "        Epoch_Anz,\n",
    "        acc,\n",
    "        val_acc ))\n",
    "\n",
    "\n",
    "def evaluate_ziffer(model):\n",
    "    \n",
    "    files = ziffer_data_files()\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 15))\n",
    "    columns = 5\n",
    "    rows = 5\n",
    "    index = 1\n",
    "    \n",
    "    for aktfile in files:\n",
    "        base = os.path.basename(aktfile)\n",
    "        target = base[0:1]\n",
    "        if target == \"N\":\n",
    "            zw1 = -1\n",
    "        else:\n",
    "            zw1 = int(target)\n",
    "        expected_class = zw1\n",
    "        image_in = Image.open(aktfile).resize((20,32))\n",
    "        test_image = np.array(image_in, dtype=np.float32)\n",
    "        img = np.reshape(test_image,[1,32,20,3])\n",
    "        \n",
    "        classesp = model.predict(img)[0]\n",
    "        classes = np.argmax(classesp)\n",
    "        if classes == 10: \n",
    "            classes = -1\n",
    "        if str(classes) != str(expected_class):\n",
    "            if index < (columns*rows):\n",
    "                fig.add_subplot(rows, columns, index)\n",
    "                plt.title(base + \"\\nExcp.: \" +   str(expected_class) + \" Pred.: \" + str(classes))  # set title\n",
    "                plt.imshow(test_image.astype(np.uint8))\n",
    "                plt.axis(\"off\")\n",
    "                index = index + 1\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def eval_model(model, history):\n",
    "    # check the complete set of zifferdata\n",
    "    if hasattr(model, 'student'):\n",
    "        model_to_eval = model.student\n",
    "    else: \n",
    "        model_to_eval = model\n",
    "\n",
    "    _, ziffer_model_accuracy = model_to_eval.evaluate(\n",
    "    x_ziffer_data, y_ziffer_data, verbose=0)\n",
    "    print('All Zifferdata accuracy:', ziffer_model_accuracy)\n",
    "\n",
    "\n",
    "    # PLOT ACCURACIES \n",
    "    plot_acc_loss(history, model._name)\n",
    "\n",
    "    evaluate_ziffer(model_to_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mflops util method\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n",
    "\n",
    "# returns the flops of the model\n",
    "def get_flops(model):\n",
    "    concrete = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete.get_concrete_function(\n",
    "        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n",
    "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name='')\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n",
    "        return flops.total_float_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "\n",
    "#### Ziffer data\n",
    "\n",
    "* The data is expected in the \"Input_dir\"\n",
    "* The first characters of the file name machtes the category.  0, 1, ... 9, 10 =category (NaN = 10)\n",
    "* Picture size must be 20x32 with 3 color channels (RGB)\n",
    "\n",
    "* The images are stored in the x_ziffer_data[]\n",
    "* The expected category for each image in the corresponding y_ziffer_data[]\n",
    "\n",
    "* The last step is a shuffle (from sklearn.utils) and split the data into training and validation data\n",
    "\n",
    "#### Ziffer data + Chars74k\n",
    "\n",
    "* The complete dataset stored in x_data[], y_data[]\n",
    "* it is shuffled too\n",
    "* the train/test of ziffer_data are the same as above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all 3 datasets \n",
    "# the ziffer dataset will be used multiple times\n",
    "x_ziffer_data, y_ziffer_data = ziffer_data([], [], nb_classes=nb_classes)\n",
    "x_ziffer_data =  np.array(x_ziffer_data)\n",
    "y_ziffer_data = np.array(y_ziffer_data).reshape(-1)\n",
    "x_ziffer_data, y_ziffer_data = shuffle(x_ziffer_data, y_ziffer_data)\n",
    "x_ziffer_train, x_ziffer_test, y_ziffer_train, y_ziffer_test = train_test_split(x_ziffer_data, y_ziffer_data, test_size=Training_Percentage)\n",
    "print(\"Ziffer  images: \", y_ziffer_data.size)\n",
    "print(\"Ziffer category count :\", np.bincount(y_ziffer_data))\n",
    "\n",
    "# th char74k dataset will be only used to lean numbers at first step\n",
    "x_data, y_data = eng_char74k_numbers([], [])\n",
    "x_data, y_data = font_char74k_numbers(x_data, y_data)\n",
    "x_data =  np.array(x_data)\n",
    "y_data = np.array(y_data).reshape(-1)\n",
    "x_data, y_data = shuffle(x_data, y_data)\n",
    "print(\"Char74  images: \", y_data.size)\n",
    "print(\"Char74 category count :\", np.bincount(y_data))\n",
    "\n",
    "\n",
    "# Split train and validation data \n",
    "##x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=Training_Percentage)\n",
    "## without char74\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ziffer_data, y_ziffer_data, test_size=Training_Percentage)\n",
    "\n",
    "# add ziffer-train/test data and shuffle again\n",
    "## without char74\n",
    "##x_train = np.concatenate((x_train, x_ziffer_train))\n",
    "##y_train = np.concatenate((y_train, y_ziffer_train))\n",
    "##x_test = np.concatenate((x_test, x_ziffer_test))\n",
    "##y_test = np.concatenate((y_test, y_ziffer_test))\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "x_test, y_test = shuffle(x_test, y_test)\n",
    "print(\"First step images (train/test): \", y_train.size, y_test.size)\n",
    "\n",
    "\n",
    "plot_dataset(x_train, y_train)\n",
    "print(\"Train category count :\", np.bincount(y_train))\n",
    "print(\"Test category count :\", np.bincount(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "\n",
    "#### Important\n",
    "* Shape of the input layer: (32, 20, 3)\n",
    "* Number of output layers: 11\n",
    "* As loss function \"sparse_categorical_crossentropy\" is choosen, as it is a categories task\n",
    "* It uses from_logits=True (same activation softmax but not defined in model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Conv2D, MaxPool2D, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "def get_post(x_in):\n",
    "    x = LeakyReLU()(x_in)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def get_block(x_in, ch_in, ch_out):\n",
    "    x = Conv2D(ch_in,\n",
    "               kernel_size=(1, 1),\n",
    "               padding='same',\n",
    "               use_bias=False)(x_in)\n",
    "    x = get_post(x)\n",
    "\n",
    "    x = DepthwiseConv2D(kernel_size=(1, 3), padding='same', use_bias=False)(x)\n",
    "    x = get_post(x)\n",
    "    x = MaxPool2D(pool_size=(2, 1),\n",
    "                  strides=(2, 1))(x) # Separable pooling\n",
    "\n",
    "    x = DepthwiseConv2D(kernel_size=(3, 1),\n",
    "                        padding='same',\n",
    "                        use_bias=False)(x)\n",
    "    x = get_post(x)\n",
    "\n",
    "    x = Conv2D(ch_out,\n",
    "               kernel_size=(2, 1),\n",
    "               strides=(1, 2),\n",
    "               padding='same',\n",
    "               use_bias=False)(x)\n",
    "    x = get_post(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def Effnet(input_shape, nb_classes, include_top=True, weights=None, activation_top=None):\n",
    "    x_in = Input(shape=input_shape)\n",
    "\n",
    "    x = get_block(x_in, 32, 64)\n",
    "    x = get_block(x, 64, 128)\n",
    "    x = get_block(x, 128, 256)\n",
    "\n",
    "    if include_top:\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(nb_classes, activation=activation_top)(x)\n",
    "\n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights, by_name=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# input_shape (32,20,3)\n",
    "# nb_classes mostly 11.\n",
    "# activation_dense None, if from_logits=True, or softmax if from_logits=False\n",
    "def CNN32C3C3C5_BN_DA(input_shape, nb_classes, activation_dense=None ):\n",
    "    model = Sequential()\n",
    "\n",
    "    model = Sequential()\n",
    "    model._name='CNN32C3C3C5_BN_DA'\n",
    "    model.add(Conv2D(32,kernel_size=3,input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32,kernel_size=5,strides=2,padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32,kernel_size=3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32,kernel_size=5,strides=2,padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(nb_classes, activation = activation_dense))\n",
    "    return model\n",
    "\n",
    "# input_shape (32,20,3)\n",
    "# nb_classes mostly 11.\n",
    "# activation_dense None, if from_logits=True, or softmax if from_logits=False\n",
    "def CNN32_Basic(input_shape, nb_classes, activation_dense=None):\n",
    "    model = Sequential()\n",
    "    model._name='CNN32_Basic'\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation=\"relu\"))\n",
    "    model.add(Dense(nb_classes, activation = activation_dense))\n",
    "    return model\n",
    "\n",
    "# input_shape (32,20,3)\n",
    "# nb_classes mostly 11.\n",
    "# activation_dense None, if from_logits=True, or softmax if from_logits=False\n",
    "def CNN32_Basic_Dropout(input_shape, nb_classes, activation_dense=None):\n",
    "    model = Sequential()\n",
    "    model._name='CNN32_Basic_Dropout'\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256,activation=\"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(nb_classes, activation = activation_dense))\n",
    "    return model\n",
    "\n",
    "# input_shape (32,20,3)\n",
    "# nb_classes mostly 11.\n",
    "# activation_dense None, if from_logits=True, or softmax if from_logits=False\n",
    "def CNN32(input_shape, nb_classes, conv=(32,64,64), dense=256, use_dropout=True, activation_dense=None):\n",
    "    conv_str = '_'.join(map(str, conv))\n",
    "    model = Sequential()\n",
    "    model._name='CNN32_C_' + conv_str + \"_D_\" + str(dense)\n",
    "    model.add(Conv2D(conv[0], (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    if (use_dropout):\n",
    "        model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(conv[1], (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    if (use_dropout):\n",
    "        model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(conv[2], (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    if (use_dropout):\n",
    "        model.add(Dropout(0.4))\n",
    "    model.add(Dense(dense,activation=\"relu\"))\n",
    "    if (use_dropout):\n",
    "        model.add(Dropout(0.4))\n",
    "    model.add(Dense(nb_classes, activation = activation_dense))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge distillation\n",
    "\n",
    "https://keras.io/examples/vision/knowledge_distillation/\n",
    "\n",
    "We need a smaller model for the EdgeAI-device. So we using knowledge distillation. It trains a teacher and student model. First the teacher model. Later the student model with combine the knowlage of the teacher model on loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation (for more output comment in summery and mflops)\n",
    "\n",
    "teacher = CNN32C3C3C5_BN_DA(input_shape, nb_classes, activation_dense=None)\n",
    "teacher._name = \"Teacher_\" + teacher._name\n",
    "#student = CNN32(input_shape, nb_classes, conv=(32, 32, 64), dense=256, use_dropout=True, activation_dense=None)\n",
    "student = Effnet(input_shape, nb_classes, activation_top=None)\n",
    "student._name = \"Student_\" + student._name\n",
    "current_model = CNN32(input_shape, nb_classes, conv=(32, 32, 64), dense=256, use_dropout=False, activation_dense=None)\n",
    "# Clone student for later comparison\n",
    "student_scratch = keras.models.clone_model(student)\n",
    "student_scratch._name = \"Student-scratch_\" + student_scratch._name\n",
    "\n",
    "\n",
    "# teacher model will later named model\n",
    "teacher.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "#teacher.summary()\n",
    "#print(f'Model {student._name} has MFLOPs :{get_flops(student)/1000000 }', )\n",
    "\n",
    "# student scratch\n",
    "student_scratch.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "student.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "current_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller._name = student._name\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# student will represent by distiller-model\n",
    "models = [teacher, student_scratch, distiller]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "The input pictures are randomly scattered for brightness, pixel shift variations and rotation angle. This is implemented with a ImageDataGenerator.\n",
    "\n",
    "Invert added for the inverse (white on black) numbers on the gas and e-meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import cv2\n",
    "import random\n",
    "\n",
    "Batch_Size = 32\n",
    "Shift_Range = 1\n",
    "Brightness_Range = 0.5\n",
    "Rotation_Angle = 5\n",
    "ZoomRange = 0.2\n",
    "ShearRange= 2\n",
    "\n",
    "def invert(imagem):\n",
    "    if (random.getrandbits(1)):\n",
    "        return (255)-imagem\n",
    "    else:\n",
    "        return imagem\n",
    "    \n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=Shift_Range, \n",
    "                             height_shift_range=Shift_Range,\n",
    "                             brightness_range=[1-Brightness_Range,1+Brightness_Range],\n",
    "                             zoom_range=[1, 1+ZoomRange],\n",
    "                             rotation_range=Rotation_Angle,\n",
    "                             channel_shift_range=1,\n",
    "                             fill_mode='nearest',\n",
    "                             shear_range=ShearRange,\n",
    "                             preprocessing_function=invert)\n",
    "print(y_train.shape)\n",
    "train_iterator = datagen.flow(x_train, y_train, batch_size=Batch_Size)\n",
    "validation_iterator = datagen.flow(x_test, y_test, batch_size=Batch_Size)\n",
    "\n",
    "plot_dataset_it(train_iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# reduzing the learning rate every epoch \n",
    "annealer = LearningRateScheduler(lambda x: 2e-3 * 0.95 ** x, verbose=0)\n",
    "\n",
    "history = [0] * len(models)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    history[i] = model.fit(train_iterator, \n",
    "                validation_data = validation_iterator, \n",
    "                batch_size=Batch_Size, \n",
    "                epochs = Epoch_Anz, \n",
    "                steps_per_epoch=len(y_train)//Batch_Size,\n",
    "                validation_steps=len(y_test)//Batch_Size,\n",
    "                callbacks=[annealer], \n",
    "                verbose=0)\n",
    "\n",
    "    printMaxHistory(history=history[i], modelname=model._name)\n",
    "    \n",
    "    # eval model on test, plot accuracy and eval on zifferdata\n",
    "    eval_model(model=model, history=history[i])\n",
    "\n",
    "#plot_val_acc(models, history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning result\n",
    " \n",
    "* Visualization of the validation results of teacher and student model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load ziffer data with NaN-class\n",
    "# Split train and validation data \n",
    "x_train, x_test, y_train, y_test = x_ziffer_train, x_ziffer_test, y_ziffer_train, y_ziffer_test\n",
    "plot_dataset(x_train, y_train)\n",
    "\n",
    "# train the models again\n",
    "train_iterator = datagen.flow(x_train, y_train, batch_size=Batch_Size)\n",
    "validation_iterator = datagen.flow(x_test, y_test, batch_size=Batch_Size)\n",
    "\n",
    "\n",
    "# because of the small dataset we need more epoches to learn\n",
    "Epoch_Anz = 100\n",
    "#(back to normal rate. learning rate was to low)\n",
    "annealer = LearningRateScheduler(lambda x: 5e-4 * 0.95 ** x, verbose=0)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    history[i] = model.fit(train_iterator, \n",
    "                validation_data = validation_iterator, \n",
    "                batch_size=Batch_Size, \n",
    "                epochs = Epoch_Anz, \n",
    "                steps_per_epoch=len(y_train)//Batch_Size,\n",
    "                validation_steps=len(y_test)//Batch_Size,\n",
    "                callbacks=[annealer], \n",
    "                verbose=0)\n",
    "    \n",
    "    printMaxHistory(history=history[i], modelname=model._name)\n",
    "    \n",
    "\n",
    "    # eval model on test, plot accuracy and eval on zifferdata\n",
    "    eval_model(model, history[i])\n",
    "\n",
    "\n",
    "#plot_val_acc(models, history)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler,EarlyStopping\n",
    "  \n",
    "callbacks = [LearningRateScheduler(lambda x: 5e-4 * 0.95 ** x, verbose=0)]\n",
    " \n",
    "_callbacks = [EarlyStopping(monitor='student_loss', patience=3)]\n",
    "_callbacks.extend(callbacks)\n",
    "\n",
    "print(_callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisation against current model training\n",
    "\n",
    "The current model training stores the model and history and trains every time on it. \n",
    "But the train and test data every will be shuffled every training. So every image is a train data over the time and the model will be much overfitted.\n",
    "\n",
    "We can see the validation accuracy is much lower than train accuracy.\n",
    "\n",
    "Another problem is, that the model does not use Dropout. So after a few epochs the training accuracy is much higher than the validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current model (small-v2) training as comparisation\n",
    "\n",
    "cur_hiso = current_model.fit(train_iterator, \n",
    "                validation_data = validation_iterator, \n",
    "                batch_size=Batch_Size, \n",
    "                epochs = 100, \n",
    "                steps_per_epoch=len(y_train)//Batch_Size,\n",
    "                validation_steps=len(y_test)//Batch_Size,\n",
    "                verbose=0)\n",
    "printMaxHistory(history=cur_hiso, modelname=current_model._name)\n",
    "\n",
    "\n",
    "\n",
    "# eval model on test, plot accuracy and eval on zifferdata\n",
    "eval_model(model=current_model, history=cur_hiso)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning an Quantization\n",
    "\n",
    "After model is trained fine it will now pruned and quantized for the EdgeAI-device.\n",
    "\n",
    "Naturally we use the distilated student model for pruning and quantization.\n",
    "\n",
    "It will quanized to uint8, means 8bit. It could be enhanced by input/ouput quantization to uint8, but the edgeAI-code must be changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pruning\n",
    "### see https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras\n",
    "print(\"Pruning...\")\n",
    "\n",
    "import tempfile\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# used for check difference to pruning\n",
    "_, baseline_model_accuracy = student.evaluate(\n",
    "    x_test, y_test, verbose=0)\n",
    "\n",
    "# beware only validation ziffer data evaluated. Not all ziffer data like above\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = x_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(student, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model_for_pruning.summary()\n",
    "\n",
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(x_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   x_train, y_train, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
    "\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "#model_for_export.summary()\n",
    "\n",
    "####### End Pruning\n",
    "\n",
    "##### Quantization of the pruned model\n",
    "FileName = TFliteNamingAndVersion + \"q.tflite\"\n",
    "\n",
    "converter    = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.experimental_enable_resource_variables = True\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "def representative_dataset():\n",
    "    for n in range(x_train[0].size):\n",
    "      data = np.expand_dims(x_train[5], axis=0)\n",
    "      yield [data.astype(np.float32)]\n",
    "         \n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter2.representative_dataset = representative_dataset\n",
    "converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter2.representative_dataset = representative_dataset\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter2.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "#converter2.inference_input_type = tf.uint8\n",
    "#converter2.inference_output_type = tf.uint8\n",
    "tflite_quant_model = converter2.convert()\n",
    "\n",
    "open(FileName, \"wb\").write(tflite_quant_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the final tflite model by hand\n",
    "\n",
    "* The pruned and quantized student model will be evaluated. This are the results we get on the edgeAI device\n",
    "* All pictures of the complete ziffer data set will be checked and all false predicted are printed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ziffer_data_files()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 15))\n",
    "columns = 5\n",
    "rows = 5\n",
    "index = 0\n",
    "\n",
    "# we use the tflite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"kndq.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "\n",
    "for aktfile in files:\n",
    "    base = os.path.basename(aktfile)\n",
    "    target = base[0:1]\n",
    "    if target == \"N\":\n",
    "        zw1 = -1\n",
    "    else:\n",
    "        zw1 = int(target)\n",
    "    expected_class = zw1\n",
    "    image_in = Image.open(aktfile).resize((20,32))\n",
    "    test_image = np.array(image_in, dtype=np.float32)\n",
    "    img = np.reshape(test_image,[1,32,20,3])\n",
    "    \n",
    "    interpreter.set_tensor(input_index, img)\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.get_tensor(output_index)\n",
    "    classesp = output[0]\n",
    "    #classesp = model.predict(img)[0]\n",
    "    classes = np.argmax(output[0])\n",
    "    if classes == 10: \n",
    "        classes = -1\n",
    "    if str(classes) != str(expected_class):\n",
    "        index = index + 1\n",
    "        if index < (columns*rows):\n",
    "            fig.add_subplot(rows, columns, index)\n",
    "            plt.title(base + \"\\nExcp.: \" +   str(expected_class) + \" Pred.: \" + str(classes))  # set title\n",
    "            plt.imshow(test_image.astype(np.uint8))\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "\n",
    "print(\"accuracy of all zifferdata: \", (len(files)-index)/len(files))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the images shows, that this are border line images, which can be interpreted as a good digit or a faulty one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
